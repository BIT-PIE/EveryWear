<div align="center">

# Human Motion Estimation with Everyday Wearables
Siqi Zhu*, [Yixuan Li*](https://yixxuan-li.github.io/), Junfu Li*, Qi Wu*,
[Zan Wang*](https://silvester.wang/), Haozhe Ma and [Wei Liang](https://liangwei-bit.github.io/web/)

<p align="center">
<a href='https://arxiv.org/abs/'><img src='https://img.shields.io/badge/ArXiv--red'></a>
<a href='https://pie-lab.cn/EveryWear/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>
</p>

<p align="center">
  <img src="docs/imgs/teaser.png" width="700" alt="teaser"/>
</p>
</div>

> ğŸŒŸ <strong>EveryWear</strong>: A lightweight and practical tool to help you easily mocap with everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras.


---

## ğŸ“‘ Table of Contents

- [ğŸŒŸ EveryWear](#-egoverse)
  - [ğŸ–¼ï¸ Demo](#ï¸-demo)
  - [ğŸ“¦ Installation](#-installation)
  - [ğŸ›  Usage](#-usage)
    - [Training](#training)
    - [Evaluation](#evaluation)
  - [ğŸ“œ References](#-references)

---

## ğŸ–¼ï¸ Demo
<div align="center">
  <img src="docs/imgs/demo_1.gif" alt="Demo">
  <img src="docs/imgs/demo_2.gif" alt="Demo">
</div>

## TODOs

- [ ] Release dataset.
- [ ] Release code.
---

## ğŸ“œ References
If you find this project helpful, please consider citing our work:
```
@article{zhu2025human,
  title={Human Motion Estimation with Everyday Wearables},
  author={Zhu, Siqi and Li, Yixuan and Li, Junfu and Wu, Qi and Wang, Zan and Ma, Haozhe and Liang, Wei},
  journal={arXiv preprint arXiv:},
  year={2025}
}
```

